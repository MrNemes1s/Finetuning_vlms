{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Document VLM Fine-tuning Module\n",
    "## Qwen2-VL 7B Fine-tuning for Document Understanding\n",
    "\n",
    "This notebook provides a comprehensive pipeline for:\n",
    "1. Processing PDF financial documents\n",
    "2. Loading and preparing Qwen2-VL 7B model\n",
    "3. Data preprocessing and validation\n",
    "4. Fine-tuning with LoRA/QLoRA\n",
    "5. Model evaluation and saving\n",
    "\n",
    "Designed for use on Runpod or other GPU-enabled environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision transformers accelerate bitsandbytes\n",
    "!pip install -q peft datasets pillow pdf2image pypdf2\n",
    "!pip install -q qwen-vl-utils evaluate scikit-learn\n",
    "!pip install -q sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PDF and Image Processing\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import PyPDF2\n",
    "\n",
    "# Model and Training\n",
    "from transformers import (\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import evaluate\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FineTuningConfig:\n",
    "    \"\"\"Configuration for fine-tuning pipeline\"\"\"\n",
    "    \n",
    "    # Model configuration\n",
    "    model_name: str = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "    use_quantization: bool = True\n",
    "    load_in_4bit: bool = True\n",
    "    load_in_8bit: bool = False\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = None\n",
    "    \n",
    "    # Training configuration\n",
    "    output_dir: str = \"./finetuned_qwen2vl\"\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 1\n",
    "    per_device_eval_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    max_grad_norm: float = 0.3\n",
    "    warmup_ratio: float = 0.03\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    \n",
    "    # Image processing\n",
    "    max_image_size: Tuple[int, int] = (1024, 1024)\n",
    "    dpi: int = 200\n",
    "    \n",
    "    # Data configuration\n",
    "    train_split: float = 0.8\n",
    "    eval_split: float = 0.1\n",
    "    test_split: float = 0.1\n",
    "    max_length: int = 512\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 100\n",
    "    eval_steps: int = 100\n",
    "    save_total_limit: int = 3\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lora_target_modules is None:\n",
    "            # Target attention and MLP layers for Qwen2-VL\n",
    "            self.lora_target_modules = [\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "            ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = FineTuningConfig()\n",
    "print(\"Configuration initialized:\")\n",
    "print(json.dumps(config.__dict__, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PDF Processing and Image Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFProcessor:\n",
    "    \"\"\"Process PDF documents and extract images\"\"\"\n",
    "    \n",
    "    def __init__(self, dpi: int = 200, max_image_size: Tuple[int, int] = (1024, 1024)):\n",
    "        self.dpi = dpi\n",
    "        self.max_image_size = max_image_size\n",
    "    \n",
    "    def validate_pdf(self, pdf_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Validate PDF and extract metadata\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                metadata = pdf_reader.metadata\n",
    "                \n",
    "                return {\n",
    "                    'valid': True,\n",
    "                    'num_pages': num_pages,\n",
    "                    'metadata': metadata,\n",
    "                    'file_size': os.path.getsize(pdf_path)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'valid': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def pdf_to_images(self, pdf_path: str, output_dir: Optional[str] = None) -> List[Image.Image]:\n",
    "        \"\"\"Convert PDF pages to images\"\"\"\n",
    "        try:\n",
    "            # Validate PDF first\n",
    "            validation = self.validate_pdf(pdf_path)\n",
    "            if not validation['valid']:\n",
    "                raise ValueError(f\"Invalid PDF: {validation['error']}\")\n",
    "            \n",
    "            # Convert to images\n",
    "            images = convert_from_path(\n",
    "                pdf_path,\n",
    "                dpi=self.dpi,\n",
    "                fmt='png'\n",
    "            )\n",
    "            \n",
    "            # Resize if needed\n",
    "            processed_images = []\n",
    "            for idx, img in enumerate(images):\n",
    "                # Resize maintaining aspect ratio\n",
    "                img.thumbnail(self.max_image_size, Image.Resampling.LANCZOS)\n",
    "                processed_images.append(img)\n",
    "                \n",
    "                # Optionally save to disk\n",
    "                if output_dir:\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    img_path = os.path.join(output_dir, f\"page_{idx+1}.png\")\n",
    "                    img.save(img_path, 'PNG')\n",
    "            \n",
    "            print(f\"✓ Converted {len(processed_images)} pages from {pdf_path}\")\n",
    "            return processed_images\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing PDF {pdf_path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def batch_process_pdfs(self, pdf_paths: List[str], output_base_dir: str = \"./processed_pdfs\") -> Dict[str, List[Image.Image]]:\n",
    "        \"\"\"Process multiple PDFs\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            pdf_name = Path(pdf_path).stem\n",
    "            output_dir = os.path.join(output_base_dir, pdf_name)\n",
    "            \n",
    "            try:\n",
    "                images = self.pdf_to_images(pdf_path, output_dir)\n",
    "                results[pdf_name] = images\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {pdf_path}: {e}\")\n",
    "                results[pdf_name] = []\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the PDF processor\n",
    "pdf_processor = PDFProcessor(dpi=config.dpi, max_image_size=config.max_image_size)\n",
    "print(\"PDF Processor initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDocumentDataset:\n",
    "    \"\"\"Dataset handler for financial documents with key field extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, config: FineTuningConfig):\n",
    "        self.config = config\n",
    "        self.data = []\n",
    "        self.key_fields_schema = None\n",
    "    \n",
    "    def load_key_fields_schema(self, schema_path: str) -> Dict:\n",
    "        \"\"\"Load JSON schema for key fields\"\"\"\n",
    "        try:\n",
    "            with open(schema_path, 'r') as f:\n",
    "                self.key_fields_schema = json.load(f)\n",
    "            \n",
    "            print(f\"✓ Loaded key fields schema with {len(self.key_fields_schema.get('fields', []))} fields\")\n",
    "            return self.key_fields_schema\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading schema: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def validate_annotations(self, annotations: Dict) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate annotations against schema\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        if not self.key_fields_schema:\n",
    "            errors.append(\"No schema loaded\")\n",
    "            return False, errors\n",
    "        \n",
    "        required_fields = [f['name'] for f in self.key_fields_schema.get('fields', []) if f.get('required', False)]\n",
    "        \n",
    "        # Check required fields\n",
    "        for field in required_fields:\n",
    "            if field not in annotations:\n",
    "                errors.append(f\"Missing required field: {field}\")\n",
    "        \n",
    "        # Check field types\n",
    "        for field_def in self.key_fields_schema.get('fields', []):\n",
    "            field_name = field_def['name']\n",
    "            if field_name in annotations:\n",
    "                expected_type = field_def.get('type', 'string')\n",
    "                value = annotations[field_name]\n",
    "                \n",
    "                if expected_type == 'number' and not isinstance(value, (int, float)):\n",
    "                    try:\n",
    "                        float(value)\n",
    "                    except:\n",
    "                        errors.append(f\"Field {field_name} should be numeric\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def add_sample(self, image: Image.Image, annotations: Dict, metadata: Optional[Dict] = None):\n",
    "        \"\"\"Add a training sample\"\"\"\n",
    "        # Validate annotations\n",
    "        is_valid, errors = self.validate_annotations(annotations)\n",
    "        if not is_valid:\n",
    "            print(f\"⚠ Validation warnings: {errors}\")\n",
    "        \n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'annotations': annotations,\n",
    "            'metadata': metadata or {},\n",
    "            'validation_errors': errors if not is_valid else []\n",
    "        }\n",
    "        \n",
    "        self.data.append(sample)\n",
    "    \n",
    "    def load_from_directory(self, data_dir: str, annotations_file: str):\n",
    "        \"\"\"Load dataset from directory with annotations\"\"\"\n",
    "        try:\n",
    "            # Load annotations\n",
    "            with open(annotations_file, 'r') as f:\n",
    "                annotations_data = json.load(f)\n",
    "            \n",
    "            # Process each annotated document\n",
    "            for item in annotations_data:\n",
    "                image_path = os.path.join(data_dir, item['image_file'])\n",
    "                if os.path.exists(image_path):\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    self.add_sample(\n",
    "                        image=image,\n",
    "                        annotations=item['fields'],\n",
    "                        metadata={'source': item.get('source', 'unknown')}\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"⚠ Image not found: {image_path}\")\n",
    "            \n",
    "            print(f\"✓ Loaded {len(self.data)} samples from {data_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_prompt(self, key_fields: List[str]) -> str:\n",
    "        \"\"\"Create extraction prompt for the model\"\"\"\n",
    "        fields_str = \", \".join(key_fields)\n",
    "        prompt = f\"\"\"Extract the following fields from this financial document: {fields_str}\n",
    "Return the information in JSON format with the exact field names as keys.\n",
    "If a field is not found, use null as the value.\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def split_dataset(self) -> Tuple[List, List, List]:\n",
    "        \"\"\"Split dataset into train, eval, and test sets\"\"\"\n",
    "        total_samples = len(self.data)\n",
    "        train_size = int(total_samples * self.config.train_split)\n",
    "        eval_size = int(total_samples * self.config.eval_split)\n",
    "        \n",
    "        # Shuffle data\n",
    "        import random\n",
    "        random.shuffle(self.data)\n",
    "        \n",
    "        train_data = self.data[:train_size]\n",
    "        eval_data = self.data[train_size:train_size + eval_size]\n",
    "        test_data = self.data[train_size + eval_size:]\n",
    "        \n",
    "        print(f\"✓ Dataset split: {len(train_data)} train, {len(eval_data)} eval, {len(test_data)} test\")\n",
    "        return train_data, eval_data, test_data\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Get dataset statistics\"\"\"\n",
    "        stats = {\n",
    "            'total_samples': len(self.data),\n",
    "            'samples_with_errors': sum(1 for s in self.data if s['validation_errors']),\n",
    "            'unique_fields': set()\n",
    "        }\n",
    "        \n",
    "        for sample in self.data:\n",
    "            stats['unique_fields'].update(sample['annotations'].keys())\n",
    "        \n",
    "        stats['unique_fields'] = list(stats['unique_fields'])\n",
    "        return stats\n",
    "\n",
    "print(\"Dataset handler initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2VLModelLoader:\n",
    "    \"\"\"Load and prepare Qwen2-VL model for fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, config: FineTuningConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def load_model(self) -> Tuple[Any, Any, Any]:\n",
    "        \"\"\"Load Qwen2-VL model with optional quantization\"\"\"\n",
    "        print(f\"Loading {self.config.model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Configure quantization\n",
    "            quantization_config = None\n",
    "            if self.config.use_quantization:\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=self.config.load_in_4bit,\n",
    "                    load_in_8bit=self.config.load_in_8bit,\n",
    "                    bnb_4bit_compute_dtype=torch.float16,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                )\n",
    "                print(f\"✓ Quantization enabled: {('4-bit' if self.config.load_in_4bit else '8-bit')}\")\n",
    "            \n",
    "            # Load processor and tokenizer\n",
    "            self.processor = AutoProcessor.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"✓ Processor and tokenizer loaded\")\n",
    "            \n",
    "            # Load model\n",
    "            self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if quantization_config else torch.float32,\n",
    "            )\n",
    "            print(\"✓ Model loaded successfully\")\n",
    "            \n",
    "            # Prepare for k-bit training if quantized\n",
    "            if quantization_config:\n",
    "                self.model = prepare_model_for_kbit_training(self.model)\n",
    "                print(\"✓ Model prepared for k-bit training\")\n",
    "            \n",
    "            # Print model memory footprint\n",
    "            if torch.cuda.is_available():\n",
    "                memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                print(f\"✓ GPU memory allocated: {memory_allocated:.2f} GB\")\n",
    "            \n",
    "            return self.model, self.processor, self.tokenizer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def apply_lora(self) -> Any:\n",
    "        \"\"\"Apply LoRA adapters to the model\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        print(\"Applying LoRA configuration...\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.lora_r,\n",
    "            lora_alpha=self.config.lora_alpha,\n",
    "            target_modules=self.config.lora_target_modules,\n",
    "            lora_dropout=self.config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"✓ LoRA applied: {trainable_params:,} trainable parameters ({100 * trainable_params / total_params:.2f}%)\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def get_model_info(self) -> Dict:\n",
    "        \"\"\"Get model information\"\"\"\n",
    "        if self.model is None:\n",
    "            return {'status': 'not_loaded'}\n",
    "        \n",
    "        info = {\n",
    "            'model_name': self.config.model_name,\n",
    "            'quantized': self.config.use_quantization,\n",
    "            'device': str(next(self.model.parameters()).device),\n",
    "            'dtype': str(next(self.model.parameters()).dtype),\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            info['gpu_memory_allocated'] = f\"{torch.cuda.memory_allocated() / 1e9:.2f} GB\"\n",
    "            info['gpu_memory_reserved'] = f\"{torch.cuda.memory_reserved() / 1e9:.2f} GB\"\n",
    "        \n",
    "        return info\n",
    "\n",
    "print(\"Model loader class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Dataset Class for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLMFinancialDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for VLM fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], processor: Any, tokenizer: Any, key_fields: List[str], max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.key_fields = key_fields\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def create_prompt(self) -> str:\n",
    "        \"\"\"Create extraction prompt\"\"\"\n",
    "        fields_str = \", \".join(self.key_fields)\n",
    "        return f\"\"\"Extract the following fields from this financial document: {fields_str}\n",
    "Return the information in JSON format.\"\"\"\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        sample = self.data[idx]\n",
    "        image = sample['image']\n",
    "        annotations = sample['annotations']\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = self.create_prompt()\n",
    "        \n",
    "        # Create target (ground truth)\n",
    "        target = json.dumps(annotations, ensure_ascii=False)\n",
    "        \n",
    "        # Process image and text\n",
    "        # For Qwen2-VL, we need to create a conversation format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": target}]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process with processor\n",
    "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Prepare labels (for causal language modeling)\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0) if \"pixel_values\" in inputs else None,\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }\n",
    "\n",
    "print(\"Custom dataset class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningPipeline:\n",
    "    \"\"\"Complete fine-tuning pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: FineTuningConfig):\n",
    "        self.config = config\n",
    "        self.model_loader = Qwen2VLModelLoader(config)\n",
    "        self.dataset = FinancialDocumentDataset(config)\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.tokenizer = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def setup(self, schema_path: str):\n",
    "        \"\"\"Setup the pipeline\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 1: Loading Model\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load model\n",
    "        self.model, self.processor, self.tokenizer = self.model_loader.load_model()\n",
    "        \n",
    "        # Apply LoRA\n",
    "        self.model = self.model_loader.apply_lora()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 2: Loading Schema\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load schema\n",
    "        self.dataset.load_key_fields_schema(schema_path)\n",
    "        \n",
    "        print(\"\\n✓ Setup complete!\\n\")\n",
    "    \n",
    "    def load_data(self, data_dir: str, annotations_file: str):\n",
    "        \"\"\"Load training data\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STEP 3: Loading Training Data\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.dataset.load_from_directory(data_dir, annotations_file)\n",
    "        \n",
    "        # Print statistics\n",
    "        stats = self.dataset.get_statistics()\n",
    "        print(f\"\\nDataset Statistics:\")\n",
    "        print(f\"  Total samples: {stats['total_samples']}\")\n",
    "        print(f\"  Samples with validation errors: {stats['samples_with_errors']}\")\n",
    "        print(f\"  Unique fields: {len(stats['unique_fields'])}\")\n",
    "    \n",
    "    def prepare_datasets(self) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "        \"\"\"Prepare train, eval, and test datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 4: Preparing Datasets\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Split data\n",
    "        train_data, eval_data, test_data = self.dataset.split_dataset()\n",
    "        \n",
    "        # Get key fields\n",
    "        key_fields = [f['name'] for f in self.dataset.key_fields_schema.get('fields', [])]\n",
    "        \n",
    "        # Create PyTorch datasets\n",
    "        train_dataset = VLMFinancialDataset(\n",
    "            train_data, self.processor, self.tokenizer, key_fields, self.config.max_length\n",
    "        )\n",
    "        eval_dataset = VLMFinancialDataset(\n",
    "            eval_data, self.processor, self.tokenizer, key_fields, self.config.max_length\n",
    "        )\n",
    "        test_dataset = VLMFinancialDataset(\n",
    "            test_data, self.processor, self.tokenizer, key_fields, self.config.max_length\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Datasets prepared\")\n",
    "        return train_dataset, eval_dataset, test_dataset\n",
    "    \n",
    "    def train(self, train_dataset: Dataset, eval_dataset: Dataset):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 5: Training\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "            gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            max_grad_norm=self.config.max_grad_norm,\n",
    "            warmup_ratio=self.config.warmup_ratio,\n",
    "            lr_scheduler_type=self.config.lr_scheduler_type,\n",
    "            logging_steps=self.config.logging_steps,\n",
    "            save_steps=self.config.save_steps,\n",
    "            eval_steps=self.config.eval_steps,\n",
    "            save_total_limit=self.config.save_total_limit,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            load_best_model_at_end=True,\n",
    "            push_to_hub=False,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            optim=\"paged_adamw_8bit\" if self.config.use_quantization else \"adamw_torch\",\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nStarting training...\")\n",
    "        print(f\"  Epochs: {self.config.num_train_epochs}\")\n",
    "        print(f\"  Batch size: {self.config.per_device_train_batch_size}\")\n",
    "        print(f\"  Gradient accumulation: {self.config.gradient_accumulation_steps}\")\n",
    "        print(f\"  Effective batch size: {self.config.per_device_train_batch_size * self.config.gradient_accumulation_steps}\")\n",
    "        print(f\"  Learning rate: {self.config.learning_rate}\\n\")\n",
    "        \n",
    "        # Train\n",
    "        self.trainer.train()\n",
    "        \n",
    "        print(\"\\n✓ Training complete!\")\n",
    "    \n",
    "    def save_model(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Save the fine-tuned model\"\"\"\n",
    "        save_path = save_path or self.config.output_dir\n",
    "        \n",
    "        print(f\"\\nSaving model to {save_path}...\")\n",
    "        \n",
    "        # Save the model\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.processor.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        # Save config\n",
    "        config_path = os.path.join(save_path, \"training_config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config.__dict__, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"✓ Model saved to {save_path}\")\n",
    "    \n",
    "    def load_finetuned_model(self, model_path: str):\n",
    "        \"\"\"Load a fine-tuned model\"\"\"\n",
    "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
    "        \n",
    "        from peft import PeftModel\n",
    "        \n",
    "        # Load base model first\n",
    "        base_model, processor, tokenizer = self.model_loader.load_model()\n",
    "        \n",
    "        # Load PEFT model\n",
    "        self.model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        print(\"✓ Fine-tuned model loaded\")\n",
    "        return self.model, self.processor, self.tokenizer\n",
    "\n",
    "print(\"Training pipeline class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Evaluate fine-tuned model performance\"\"\"\n",
    "    \n",
    "    def __init__(self, model, processor, tokenizer):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, image: Image.Image, key_fields: List[str]) -> Dict:\n",
    "        \"\"\"Make prediction on a single image\"\"\"\n",
    "        # Create prompt\n",
    "        fields_str = \", \".join(key_fields)\n",
    "        prompt = f\"\"\"Extract the following fields from this financial document: {fields_str}\n",
    "Return the information in JSON format.\"\"\"\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process\n",
    "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=[image],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.processor.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )[0]\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        try:\n",
    "            # Try to find JSON in the response\n",
    "            import re\n",
    "            json_match = re.search(r'\\{.*\\}', generated_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                result = json.loads(json_match.group())\n",
    "            else:\n",
    "                result = {\"error\": \"No JSON found in response\", \"raw_text\": generated_text}\n",
    "        except json.JSONDecodeError:\n",
    "            result = {\"error\": \"Invalid JSON\", \"raw_text\": generated_text}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def evaluate_dataset(self, test_dataset: Dataset, key_fields: List[str]) -> Dict:\n",
    "        \"\"\"Evaluate on test dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = []\n",
    "        correct_fields = {field: 0 for field in key_fields}\n",
    "        total_samples = len(test_dataset)\n",
    "        \n",
    "        for idx in range(total_samples):\n",
    "            sample = test_dataset.data[idx]\n",
    "            image = sample['image']\n",
    "            ground_truth = sample['annotations']\n",
    "            \n",
    "            # Predict\n",
    "            prediction = self.predict(image, key_fields)\n",
    "            \n",
    "            # Compare\n",
    "            sample_result = {\n",
    "                'ground_truth': ground_truth,\n",
    "                'prediction': prediction,\n",
    "                'correct_fields': {}\n",
    "            }\n",
    "            \n",
    "            for field in key_fields:\n",
    "                if field in prediction and field in ground_truth:\n",
    "                    # Normalize for comparison\n",
    "                    pred_val = str(prediction[field]).strip().lower()\n",
    "                    true_val = str(ground_truth[field]).strip().lower()\n",
    "                    \n",
    "                    is_correct = pred_val == true_val\n",
    "                    sample_result['correct_fields'][field] = is_correct\n",
    "                    \n",
    "                    if is_correct:\n",
    "                        correct_fields[field] += 1\n",
    "            \n",
    "            results.append(sample_result)\n",
    "            \n",
    "            print(f\"  Evaluated sample {idx + 1}/{total_samples}\", end='\\r')\n",
    "        \n",
    "        # Calculate metrics\n",
    "        field_accuracy = {field: correct / total_samples for field, correct in correct_fields.items()}\n",
    "        overall_accuracy = sum(field_accuracy.values()) / len(field_accuracy) if field_accuracy else 0\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'total_samples': total_samples,\n",
    "            'overall_accuracy': overall_accuracy,\n",
    "            'field_accuracy': field_accuracy,\n",
    "            'detailed_results': results\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\\nEvaluation Results:\")\n",
    "        print(f\"  Total samples: {total_samples}\")\n",
    "        print(f\"  Overall accuracy: {overall_accuracy:.2%}\")\n",
    "        print(f\"\\n  Per-field accuracy:\")\n",
    "        for field, acc in field_accuracy.items():\n",
    "            print(f\"    {field}: {acc:.2%}\")\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def save_evaluation_results(self, results: Dict, output_path: str):\n",
    "        \"\"\"Save evaluation results to file\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2, default=str)\n",
    "        print(f\"\\n✓ Evaluation results saved to {output_path}\")\n",
    "\n",
    "print(\"Evaluator class defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example Usage Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Complete end-to-end pipeline\n",
    "\n",
    "def run_complete_pipeline(\n",
    "    pdf_files: List[str],\n",
    "    schema_path: str,\n",
    "    annotations_file: str,\n",
    "    output_dir: str = \"./finetuned_qwen2vl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline for fine-tuning Qwen2-VL on financial documents\n",
    "    \n",
    "    Args:\n",
    "        pdf_files: List of PDF file paths\n",
    "        schema_path: Path to JSON schema file with key fields\n",
    "        annotations_file: Path to annotations JSON file\n",
    "        output_dir: Directory to save fine-tuned model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Update config\n",
    "    config.output_dir = output_dir\n",
    "    \n",
    "    # Step 1: Process PDFs\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PDF PROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pdf_processor = PDFProcessor(dpi=config.dpi, max_image_size=config.max_image_size)\n",
    "    processed_images = pdf_processor.batch_process_pdfs(pdf_files)\n",
    "    \n",
    "    # Step 2: Initialize pipeline\n",
    "    pipeline = FineTuningPipeline(config)\n",
    "    pipeline.setup(schema_path)\n",
    "    \n",
    "    # Step 3: Load data\n",
    "    data_dir = \"./processed_pdfs\"  # Directory where images were saved\n",
    "    pipeline.load_data(data_dir, annotations_file)\n",
    "    \n",
    "    # Step 4: Prepare datasets\n",
    "    train_dataset, eval_dataset, test_dataset = pipeline.prepare_datasets()\n",
    "    \n",
    "    # Step 5: Train\n",
    "    pipeline.train(train_dataset, eval_dataset)\n",
    "    \n",
    "    # Step 6: Save model\n",
    "    pipeline.save_model()\n",
    "    \n",
    "    # Step 7: Evaluate\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    evaluator = ModelEvaluator(pipeline.model, pipeline.processor, pipeline.tokenizer)\n",
    "    key_fields = [f['name'] for f in pipeline.dataset.key_fields_schema.get('fields', [])]\n",
    "    eval_results = evaluator.evaluate_dataset(test_dataset, key_fields)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_output_path = os.path.join(output_dir, \"evaluation_results.json\")\n",
    "    evaluator.save_evaluation_results(eval_results, eval_output_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nModel saved to: {output_dir}\")\n",
    "    print(f\"Evaluation results: {eval_output_path}\")\n",
    "    print(f\"Overall accuracy: {eval_results['overall_accuracy']:.2%}\")\n",
    "    \n",
    "    return pipeline, eval_results\n",
    "\n",
    "print(\"\\n✓ Complete pipeline function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Create a sample schema file\n",
    "sample_schema = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"invoice_number\", \"type\": \"string\", \"required\": True},\n",
    "        {\"name\": \"date\", \"type\": \"string\", \"required\": True},\n",
    "        {\"name\": \"total_amount\", \"type\": \"number\", \"required\": True},\n",
    "        {\"name\": \"vendor_name\", \"type\": \"string\", \"required\": True},\n",
    "        {\"name\": \"tax_amount\", \"type\": \"number\", \"required\": False},\n",
    "        {\"name\": \"currency\", \"type\": \"string\", \"required\": False}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save sample schema\n",
    "with open(\"sample_schema.json\", \"w\") as f:\n",
    "    json.dump(sample_schema, f, indent=2)\n",
    "\n",
    "print(\"Sample schema created: sample_schema.json\")\n",
    "print(json.dumps(sample_schema, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Create sample annotations file format\n",
    "sample_annotations = [\n",
    "    {\n",
    "        \"image_file\": \"document1/page_1.png\",\n",
    "        \"source\": \"invoice_dataset\",\n",
    "        \"fields\": {\n",
    "            \"invoice_number\": \"INV-2024-001\",\n",
    "            \"date\": \"2024-01-15\",\n",
    "            \"total_amount\": 1234.56,\n",
    "            \"vendor_name\": \"ACME Corporation\",\n",
    "            \"tax_amount\": 123.45,\n",
    "            \"currency\": \"USD\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"image_file\": \"document2/page_1.png\",\n",
    "        \"source\": \"invoice_dataset\",\n",
    "        \"fields\": {\n",
    "            \"invoice_number\": \"INV-2024-002\",\n",
    "            \"date\": \"2024-01-16\",\n",
    "            \"total_amount\": 5678.90,\n",
    "            \"vendor_name\": \"Tech Solutions Inc\",\n",
    "            \"tax_amount\": 567.89,\n",
    "            \"currency\": \"USD\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save sample annotations\n",
    "with open(\"sample_annotations.json\", \"w\") as f:\n",
    "    json.dump(sample_annotations, f, indent=2)\n",
    "\n",
    "print(\"Sample annotations created: sample_annotations.json\")\n",
    "print(json.dumps(sample_annotations[:1], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quick Test - Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Quick test on a single image\n",
    "# Uncomment and modify paths to test\n",
    "\n",
    "\"\"\"\n",
    "# Load model\n",
    "pipeline = FineTuningPipeline(config)\n",
    "pipeline.load_finetuned_model(\"./finetuned_qwen2vl\")\n",
    "\n",
    "# Load test image\n",
    "test_image = Image.open(\"path/to/test/invoice.png\")\n",
    "\n",
    "# Define fields to extract\n",
    "key_fields = [\"invoice_number\", \"date\", \"total_amount\", \"vendor_name\"]\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = ModelEvaluator(pipeline.model, pipeline.processor, pipeline.tokenizer)\n",
    "\n",
    "# Make prediction\n",
    "result = evaluator.predict(test_image, key_fields)\n",
    "\n",
    "print(\"\\nExtracted Fields:\")\n",
    "print(json.dumps(result, indent=2))\n",
    "\"\"\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Main Execution (Uncomment to run)\n",
    "\n",
    "To run the complete pipeline, uncomment the code below and provide your data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN EXECUTION\n",
    "# Uncomment and modify paths to run the complete pipeline\n",
    "\n",
    "\"\"\"\n",
    "# Define your data paths\n",
    "PDF_FILES = [\n",
    "    \"./data/invoice1.pdf\",\n",
    "    \"./data/invoice2.pdf\",\n",
    "    # Add more PDF files\n",
    "]\n",
    "\n",
    "SCHEMA_PATH = \"./sample_schema.json\"\n",
    "ANNOTATIONS_FILE = \"./sample_annotations.json\"\n",
    "OUTPUT_DIR = \"./finetuned_qwen2vl\"\n",
    "\n",
    "# Run the complete pipeline\n",
    "pipeline, eval_results = run_complete_pipeline(\n",
    "    pdf_files=PDF_FILES,\n",
    "    schema_path=SCHEMA_PATH,\n",
    "    annotations_file=ANNOTATIONS_FILE,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY TO USE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis notebook is ready to be imported and run on Runpod.\")\n",
    "print(\"\\nTo use:\")\n",
    "print(\"1. Upload this notebook to your Runpod instance\")\n",
    "print(\"2. Prepare your data (PDFs, schema, annotations)\")\n",
    "print(\"3. Update the paths in the Main Execution cell\")\n",
    "print(\"4. Run all cells\")\n",
    "print(\"\\nThe fine-tuned model will be saved to the output directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
